{
  "schemaVersion": "0.1.0",
  "name": "ollama-chat-api",
  "version": "1.0.0",
  "description": "FastAPI service that provides OpenAI-compatible chat completions using Ollama",
  "author": "Your Name",
  "license": "MIT",
  "homepage": "https://github.com/yourusername/ollama-chat-api",
  "repository": {
    "type": "git",
    "url": "https://github.com/yourusername/ollama-chat-api.git"
  },
  "capabilities": {
    "resources": true,
    "tools": true,
    "prompts": true
  },
  "server": {
    "command": "uv",
    "args": ["run", "python", "-m", "mcp_server"],
    "env": {
      "OLLAMA_BASE_URL": "http://localhost:11434",
      "MCP_SERVER_PORT": "4891"
    }
  },
  "client": {
    "name": "vscode",
    "version": ">=1.80.0"
  },
  "transport": {
    "type": "stdio"
  },
  "resources": [
    {
      "name": "chat_completions",
      "description": "OpenAI-compatible chat completions endpoint",
      "uri": "http://localhost:4891/v1/chat/completions",
      "mimeType": "application/json"
    },
    {
      "name": "health",
      "description": "Health check endpoint",
      "uri": "http://localhost:4891/health",
      "mimeType": "application/json"
    },
    {
      "name": "models",
      "description": "List available models",
      "uri": "http://localhost:4891/v1/models",
      "mimeType": "application/json"
    }
  ],
  "tools": [
    {
      "name": "chat_completion",
      "description": "Generate chat completion using Ollama models",
      "inputSchema": {
        "type": "object",
        "properties": {
          "model": {
            "type": "string",
            "description": "Model to use for completion",
            "default": "llama2"
          },
          "messages": {
            "type": "array",
            "description": "Array of chat messages",
            "items": {
              "type": "object",
              "properties": {
                "role": {
                  "type": "string",
                  "enum": ["system", "user", "assistant"]
                },
                "content": {
                  "type": "string"
                }
              },
              "required": ["role", "content"]
            }
          },
          "temperature": {
            "type": "number",
            "description": "Sampling temperature (0-2)",
            "minimum": 0,
            "maximum": 2,
            "default": 0.7
          },
          "max_tokens": {
            "type": "integer",
            "description": "Maximum number of tokens to generate",
            "minimum": 1,
            "default": 150
          },
          "stream": {
            "type": "boolean",
            "description": "Whether to stream the response",
            "default": false
          }
        },
        "required": ["model", "messages"]
      }
    },
    {
      "name": "health_check",
      "description": "Check the health status of the Ollama Chat API",
      "inputSchema": {
        "type": "object",
        "properties": {}
      }
    },
    {
      "name": "list_models",
      "description": "List available Ollama models",
      "inputSchema": {
        "type": "object",
        "properties": {}
      }
    },
    {
      "name": "draft_post",
      "description": "Generate a Quarto blog post draft using Ollama",
      "inputSchema": {
        "type": "object",
        "properties": {
          "topic": {
            "type": "string",
            "description": "The topic for the blog post draft"
          },
          "model": {
            "type": "string",
            "description": "The model to use for generation",
            "default": "mistral:7b"
          },
          "blog_folder": {
            "type": "string",
            "description": "Target folder for blog posts",
            "default": "posts"
          }
        },
        "required": ["topic"]
      }
    }
  ],
  "prompts": [
    {
      "name": "system_prompt",
      "description": "Default system prompt for chat completions",
      "arguments": [
        {
          "name": "role",
          "description": "The role context for the assistant",
          "type": "string",
          "default": "helpful assistant"
        }
      ]
    },
    {
      "name": "code_assistant",
      "description": "System prompt for code-related tasks",
      "arguments": [
        {
          "name": "language",
          "description": "Programming language context",
          "type": "string",
          "default": "python"
        }
      ]
    }
  ],
  "configuration": {
    "properties": {
      "ollama_base_url": {
        "type": "string",
        "description": "Base URL for Ollama API",
        "default": "http://localhost:11434"
      },
      "server_port": {
        "type": "integer",
        "description": "Port for the FastAPI server",
        "default": 4891
      },
      "default_model": {
        "type": "string",
        "description": "Default model to use for completions",
        "default": "llama2"
      },
      "max_timeout": {
        "type": "integer",
        "description": "Maximum timeout for API requests (seconds)",
        "default": 120
      }
    }
  }
}
